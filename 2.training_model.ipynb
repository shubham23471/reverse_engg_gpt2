{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# simplest data to work with : tiny shakespare \n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "data = text[:1000]\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198, 198, 5962, 22307, 25, 198, 1639, 389, 477, 12939, 2138, 284, 4656, 621, 284, 1145, 680, 30, 198, 198, 3237, 25, 198, 4965, 5634, 13, 12939, 13, 198, 198, 5962, 22307, 25, 198, 5962, 11, 345, 760, 327, 1872, 385, 1526, 28599, 318, 4039, 4472, 284, 262, 661, 13, 198, 198, 3237, 25, 198, 1135, 760, 470, 11, 356, 760, 470, 13, 198, 198, 5962, 22307, 25, 198, 5756, 514, 1494, 683, 11, 290, 356, 1183, 423, 11676, 379, 674, 898, 2756, 13, 198, 3792, 470, 257, 15593, 30, 198, 198, 3237, 25, 198, 2949, 517, 3375, 319, 470, 26, 1309, 340, 307, 1760, 25, 1497, 11, 1497, 0, 198, 198, 12211, 22307, 25, 198, 3198, 1573, 11, 922, 4290, 13, 198, 198, 5962, 22307, 25, 198, 1135, 389, 17830, 3595, 4290, 11, 262, 1458, 1173, 1547, 922, 13, 198, 2061, 4934, 969, 5036, 896, 319, 561, 26958, 514, 25, 611, 484, 198, 19188, 7800, 514, 475, 262, 48713, 414, 11, 981, 340, 547, 198, 1929, 4316, 462, 11, 356, 1244, 4724, 484, 22598, 514, 31533, 306, 26, 198, 4360, 484, 892, 356, 389, 1165, 13674, 25, 262, 10904, 1108, 326, 198, 2001, 42267, 514, 11, 262, 2134, 286, 674, 24672, 11, 318, 355, 281, 198, 24807, 284, 1948, 786, 511, 20038, 26, 674, 198, 82, 13712, 590, 318, 257, 4461, 284, 606, 3914, 514, 15827, 428, 351, 198, 454, 279, 7938, 11, 304, 260, 356, 1716, 374, 1124, 25, 329, 262, 11858, 760, 314, 198, 47350, 428, 287, 16460, 329, 8509, 11, 407, 287, 24613, 329, 15827, 13, 628]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(data)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
       "        [ 5120,   597,  2252,    11,  3285,   502],\n",
       "        [ 2740,    13,   198,   198,  3237,    25],\n",
       "        [  198,  5248,   461,    11,  2740,    13]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we want to take these tokens and forward them in our GPT model \n",
    "# to do that we have reshape our long list of token sequence into a (B, T) tensor \n",
    "# before that tha shape our forward function takes\n",
    "# take an example of 24 tokens to understand this\n",
    "\n",
    "import torch\n",
    "buf = torch.tensor(tokens[:24])\n",
    "x = buf.view(4, 6)\n",
    "x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, that's our input to the transformer. Now, we also need labels to calucate the loss function. \n",
    "- For this we can write some code in the forward pass of the GPT model, as we know the next sequence is just the right of us. But the problem with that approch is that the for the token 13 (the last toke) we don't have the label information. \n",
    "- So Andrej fav way to do this, is that we will create the another tensor label of exact same size as x but it contains the labels at every position. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
       "         [ 5120,   597,  2252,    11,  3285,   502],\n",
       "         [ 2740,    13,   198,   198,  3237,    25],\n",
       "         [  198,  5248,   461,    11,  2740,    13]]),\n",
       " tensor([[22307,    25,   198,  8421,   356,  5120],\n",
       "         [  597,  2252,    11,  3285,   502,  2740],\n",
       "         [   13,   198,   198,  3237,    25,   198],\n",
       "         [ 5248,   461,    11,  2740,    13,   198]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so, in the buf we get the last token + 1 and then index our x and y \n",
    "buf = torch.tensor(tokens[:24 + 1 ])\n",
    "x = buf[:-1].view(4, 6) \n",
    "y = buf[1:].view(4, 6) \n",
    "x, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we go back to our script and create a very simpel dataloader object to load these tokens, feed them to transformer and calculate the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- \n",
    "\n",
    "Now, we want to calculate the loss function and to do so we have to modify the forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using device: cuda\n",
    "Only loading first 1k charchters as model input\n",
    "logits shape torch.Size([4, 32, 50257])\n",
    "loss tensor(11.0371, device='cuda:0', grad_fn=<NllLossBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9897725689953638e-05, -10.82490511970208)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "1/ 50257, -math.log(50257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"step {i}, loss: {loss.item()}\")\n",
    "\n",
    "# when we use this we are overfitting on the one batch x that we are passing again and again.\n",
    "# lr = 3e-4 : is like good standard learning rate for early debugging stage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we don't want to overfit our model on that one batch. We want to create the a data loader which makes sure that we are getting a fresh batch. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num of batches in a single epoch of iterating over a single dataset\n",
    "- So 1 epoch : how many unique batches do we output/iterate over before we loop back begining of the document and start reading it again. \n",
    "1000/ (4 * 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 8714)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000 // (4 * 32), 1115394 // (4 * 32) # 1115394: num of charachter but we have to use the num \n",
    "# of tokens\n",
    "# So "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch => num of tokens // (B, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktokener have roughly about 1 to 3 compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using device: cuda\n",
    "# loaded 338025 tokens\n",
    "# 1 epoch = 2640 batches\n",
    "# step 0, loss: 11.08803939819336\n",
    "# step 1, loss: 9.717626571655273\n",
    "# step 2, loss: 8.71585750579834\n",
    "# step 3, loss: 8.921952247619629\n",
    "# step 4, loss: 8.44729995727539\n",
    "# step 5, loss: 8.131488800048828\n",
    "# step 6, loss: 9.005425453186035\n",
    "# step 7, loss: 8.671504020690918\n",
    "# step 8, loss: 8.075515747070312\n",
    "# step 9, loss: 7.828042984008789\n",
    "# step 10, loss: 8.26616096496582\n",
    "# step 11, loss: 7.185250282287598\n",
    "# step 12, loss: 7.63140869140625\n",
    "# step 13, loss: 7.262790679931641\n",
    "# step 14, loss: 7.483865261077881\n",
    "# step 15, loss: 7.127208232879639\n",
    "# step 16, loss: 7.217015743255615\n",
    "# step 17, loss: 8.252432823181152\n",
    "# step 18, loss: 7.073562145233154\n",
    "# step 19, loss: 7.671370029449463\n",
    "# step 20, loss: 7.384253025054932\n",
    "# step 21, loss: 7.580746173858643\n",
    "# step 22, loss: 6.214944839477539\n",
    "# step 23, loss: 6.54738712310791\n",
    "# step 24, loss: 6.535855293273926\n",
    "# step 25, loss: 6.208362579345703\n",
    "# step 26, loss: 6.298552989959717\n",
    "# step 27, loss: 7.4446821212768555\n",
    "# step 28, loss: 6.870301723480225\n",
    "# step 29, loss: 6.647202968597412\n",
    "# step 30, loss: 6.902033805847168\n",
    "# step 31, loss: 6.967312335968018\n",
    "# step 32, loss: 6.967711448669434\n",
    "# step 33, loss: 6.67465353012085\n",
    "# step 34, loss: 7.907723426818848\n",
    "# step 35, loss: 7.669620513916016\n",
    "# step 36, loss: 7.351398944854736\n",
    "# step 37, loss: 7.470437049865723\n",
    "# step 38, loss: 7.569716453552246\n",
    "# step 39, loss: 7.141706943511963\n",
    "# step 40, loss: 7.261336803436279\n",
    "# step 41, loss: 6.442892551422119\n",
    "# step 42, loss: 6.642857551574707\n",
    "# step 43, loss: 6.760526180267334\n",
    "# step 44, loss: 6.7352776527404785\n",
    "# step 45, loss: 6.635310173034668\n",
    "# step 46, loss: 5.62755012512207\n",
    "# step 47, loss: 5.949457168579102\n",
    "# step 48, loss: 6.788867473602295\n",
    "# step 49, loss: 6.460808753967285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug while loading the weight from hugging face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/projects/pytorch-concepts/torchenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model_hf = GPT2LMHeadModel.from_pretrained('gpt2') \n",
    "sd_hf = model_hf.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 768])\n",
      "torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(sd_hf['lm_head.weight'].shape)\n",
    "print(sd_hf[\"transformer.wte.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.wte : is the token embedding at the bottom of the transformer\n",
    "# lm_head : is the language modeling head at the top of the transformer\n",
    "        #  this is taking the 768 channels of the transformer and up scale that to \n",
    "        #   50257 to get the logits for the next token\n",
    "\n",
    "# both of these are 2-dim tensor and there shape is identical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing there every single element\n",
    "(sd_hf['lm_head.weight'] == sd_hf['transformer.wte.weight']).all()\n",
    "# as you can see that all the elemets are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130897059688403\n",
      "130897059688403\n"
     ]
    }
   ],
   "source": [
    "# checking the data pointer \n",
    "print(sd_hf['lm_head.weight'].data_ptr())\n",
    "print(sd_hf['transformer.wte.weight'].data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see they have the same pointer to data as well. \n",
    "# so what's happending here is that they have same weight are sharing b/w these two\n",
    "\n",
    "# look the 3.4 Embedding and softmax section of attention all you need paper\n",
    "# The original ideas comes from the paper: Using output embedding to Language models\n",
    "\n",
    "# intiutive idea\n",
    "# You want these two matrices to behave similar in the following sense\n",
    "# if two tokens are very similar semantically like one of them is a lowercase and other \n",
    "# is all uppercase or the same token in a different langauage or something like that. \n",
    "\n",
    "# If you have similarity b/w two token presumbly you would expect that they are near by \n",
    "# in the token embedding space but in the exact same way you'd except that if you have two \n",
    "# tokens that are similar semantically you'd except them to get same probablities at the \n",
    "# output of the transfomer because they are semantically similar. \n",
    "\n",
    "# so both positions in the transfomers at the very bottom and at the top have this property \n",
    "# that similar tokens should have similar embeddings.\n",
    "\n",
    "# They also observed (in the paper) if you look at the output embeddings they also behaves \n",
    "# like word embeddings \n",
    "\n",
    "# so basicalyy wte is used twice \n",
    "# 1. at the bottom of the transformer \n",
    "# 2. at the top of the transformer\n",
    "# and In the backward pass we will get gradient contributions from both branches and \n",
    "# these gradients will add up on wte tensor\n",
    "# so we will get contribution from the classifier layer and also at the bottom of the \n",
    "# transfomer \n",
    "\n",
    "# other than this this also helps in computations. as you can these are lot of \n",
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38597376"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50257 * 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rought like 40M parameters are shared "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So that's the change we want to make in out code. is to share wte \n",
    "--- \n",
    "- after changing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sing device: cuda\n",
    "# loaded 338025 tokens\n",
    "# 1 epoch = 2640 batches\n",
    "# step 0, loss: 10.969259262084961\n",
    "# step 1, loss: 9.712841033935547\n",
    "# step 2, loss: 9.00924015045166\n",
    "# step 3, loss: 9.145369529724121\n",
    "# step 4, loss: 8.583311080932617\n",
    "# step 5, loss: 8.283556938171387\n",
    "# step 6, loss: 8.973587989807129\n",
    "# step 7, loss: 8.775681495666504\n",
    "# step 8, loss: 8.148415565490723\n",
    "# step 9, loss: 7.83228063583374\n",
    "# step 10, loss: 8.298928260803223\n",
    "# step 11, loss: 7.384541034698486\n",
    "# step 12, loss: 7.6895294189453125\n",
    "# step 13, loss: 7.419340133666992\n",
    "# step 14, loss: 7.567596435546875\n",
    "# step 15, loss: 7.4096784591674805\n",
    "# step 16, loss: 7.3226213455200195\n",
    "# step 17, loss: 8.266316413879395\n",
    "# step 18, loss: 7.1639533042907715\n",
    "# step 19, loss: 7.797882556915283\n",
    "# step 20, loss: 7.549393177032471\n",
    "# step 21, loss: 7.749938011169434\n",
    "# step 22, loss: 6.446366786956787\n",
    "# step 23, loss: 6.835708141326904\n",
    "# step 24, loss: 6.883994102478027\n",
    "# step 25, loss: 6.704035758972168\n",
    "# step 26, loss: 6.787974834442139\n",
    "# step 27, loss: 7.700334072113037\n",
    "# step 28, loss: 7.19415807723999\n",
    "# step 29, loss: 7.0182342529296875\n",
    "# step 30, loss: 6.9842987060546875\n",
    "# step 31, loss: 7.323225498199463\n",
    "# step 32, loss: 7.172547340393066\n",
    "# step 33, loss: 7.044727325439453\n",
    "# step 34, loss: 7.917436122894287\n",
    "# step 35, loss: 7.7980523109436035\n",
    "# step 36, loss: 7.7756428718566895\n",
    "# step 37, loss: 7.733654499053955\n",
    "# step 38, loss: 8.064729690551758\n",
    "# step 39, loss: 7.523628234863281\n",
    "# step 40, loss: 7.474881172180176\n",
    "# step 41, loss: 7.044745922088623\n",
    "# step 42, loss: 7.131816387176514\n",
    "# step 43, loss: 7.149776458740234\n",
    "# step 44, loss: 7.066969394683838\n",
    "# step 45, loss: 7.148806095123291\n",
    "# step 46, loss: 6.268684387207031\n",
    "# step 47, loss: 6.4054179191589355\n",
    "# step 48, loss: 6.962234020233154\n",
    "# step 49, loss: 6.824957847595215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Fixing model Intialization\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the GPT2 and GPT3 paper are not clear about the intinilzation of the weighs. So we will look the GPT2 code.\n",
    "- Along with the intinalzation from the code. when you look at the GPT2 paper under take 2. \"we scale the weights of residual layers at initialization by a factor of 1/ sqrt(N) where N is the number of residual layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.9437)\n"
     ]
    }
   ],
   "source": [
    "# motivation on the second point on what mean in the paper. \n",
    "# you start with zero in the residual stream \\\n",
    "# if you look the code where we added the residuals \n",
    "    # def forward(self, x): of class Block\n",
    "    #     x = x + self.attn(self.ln_1(x))\n",
    "    #     x = x + self.mlp(self.ln_2(x))\n",
    "# due to this addition the varience of these activations in \n",
    "# residual stream grows \n",
    "\n",
    "x = torch.zeros(768) # this is the residual stream \n",
    "n = 100 # eg: 100 layers \n",
    "\n",
    "for i in range(n):\n",
    "    # randn: gives us the normal distribution with \n",
    "    # mean = 0 and std =1 \n",
    "    x += torch.randn(768)\n",
    "\n",
    "print(x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and due to this addtion if you all the std() at the end \n",
    "# it grows to 10.\n",
    "# and the scaling factor that they used in the paper is exactly compensates \n",
    "# for that growth. \n",
    "\n",
    "# Now, if we scale down every one of these contributions by 1/sqrt(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0184)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(768) # this is the residual stream \n",
    "n = 100 # eg: 100 layers \n",
    "for i in range(n):\n",
    "    # randn: gives us the normal distribution with \n",
    "    # mean = 0 and std =1 \n",
    "    x += n ** -0.5 * torch.randn(768)\n",
    "print(x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, if we scale in this then you can see we got 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So this is the way to control the growth of activations inside the residual stream in the forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to initilizat the weight at the end of each block so the .c_proj(x) layer \n",
    "# so we go where have intinilized c_proj i.e \n",
    "# to do this we go to CausalSelfAttention --> __init__ \n",
    "\n",
    "        # self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "#  NANOGPT_SCALE_INIT this will act like as a flag to set the scaling in std\n",
    "            # if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "            #     std *= (2 * self.config.n_layer) ** -0.5\n",
    "# (what we discuss above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "#### So at this point we have intilized the GPT2 model correctly, we have created the data loader and we can train. \n",
    "Now Comes the fun part we can speed up the training by a lot\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to break the vscode script into an interactive ipython shell\n",
    "# import code ; code.interact(local=locals())\n",
    "\n",
    "# do that can check the dtype of the logits \n",
    "# logits.dtype >>> torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so by default in pytorch when you create the tensors, and this is the case \n",
    "# for all activation and the paramets fo the NN \n",
    "# by default everything is in float32 \n",
    "# >>>> that means everything is using the float representation that has 32 bits\n",
    "# (1 byte = 8 bits)\n",
    "\n",
    "# and that's a quite a bit of memory specially for deep learning workloads \n",
    "# and deep learnig workflow can torelate small precisions but this is not true for \n",
    "# all computational workflows\n",
    "\n",
    "\n",
    "# FP32 --> 19.5 TFOPS meaning \n",
    "# we means we can do 19.5 trillion operations \n",
    "\n",
    "# int8 is used for inference but not for training \n",
    "\n",
    "# In addition to that if all of these number have fewer bits of representation. \n",
    "# It's going to much easier to move them around and that's where we start to \n",
    "# get into memeory bandwidth and the memory of the model. \n",
    "\n",
    "# Not only we have the finite capacity of number of bits that our GPU can store (GPU memory)\n",
    "# but in addition to that there is speed with which you can access this memory (Memory Bandwidth)\n",
    "\n",
    "# And, most of the Deep learning application are memory bound. \n",
    "# our tensor are sitting idel waiting for data to come. \n",
    "# so, if you are at 60% data utilization that means we are doing it quite well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my spec \n",
    "# RTX 3090, 24 GB \n",
    "# CUDA Cores: 10,496\n",
    "# Tensor Cores: 328\n",
    "# Ray Tracing Cores: 82\n",
    "# Stream Multiprocessors (SM): 82, with 128 CUDA cores per SM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Looking at the Tensor Float 32 [TF32]\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are tensor cores???\n",
    "# they are intruction \n",
    "# so everytime you have a task say matmul\n",
    "# they get broken into 4x4 multiply intruction \n",
    "\n",
    "# so if you look at the the entire transformer is just the matix multiply\n",
    "# In our network, the 124M the 768 x 50527 this operation happend in the last \n",
    "# classification layer and this dominates everything else that is happening in the NN\n",
    "\n",
    "\n",
    "# FP32 is th 32 bits \n",
    "# and, TF32 is the exact same bits : (we have one sign bit and 8 exponent bit)\n",
    "# but the mantissa get cropped. so basically we have 19 bits instead of 32 bits\n",
    "# because the last 13 bits get dropped and all of this is internal to the instruction . \n",
    "# that means none of our pytorch code will change \n",
    "\n",
    "# this speed up comes at the cost that we are reducing the presion. \n",
    "# our accumulate is still an FP32, our output is FP32 , input are FP32\n",
    "# but internally things get truncated to perform these operation faster. \n",
    "# But, when you train with this you can't really tell the difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your CPU is just scheduling some work on GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changes \n",
    "#  DataLoaderLite(B=4, T=32) to DataLoaderLite(B=16, T=1024)\n",
    "#>>>> torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.07 GiB. GPU 0 has a total capacity of 23.70 GiB of which 267.19 MiB is free. Including non-PyTorch memory, this process has 23.15 GiB memory in use. Of the allocated memory 21.75 GiB is allocated by PyTorch, and 270.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "\n",
    "#DataLoaderLite(B=8, T=1024) > 550ms   \n",
    "# memory consumption was 20.5GB on GPU\n",
    "\n",
    "# sso this is baseline in float 32 \n",
    "# using device: cuda\n",
    "# loaded 338025 tokens\n",
    "# 1 epoch = 41 batches\n",
    "# step 49, loss: 5.919715404510498, dt: 541.97ms, tok/sec: 15115.280568754155\n",
    "\n",
    "# Andrej suggestion \n",
    "# you want to max out the batch_size that you can fit in your GPU\n",
    "# and use nice numbers: numbers that have lots of powers of two in them \n",
    "# so 16, 8, 24, 32 and so on \n",
    "\n",
    "# we might change the batch size but token_per_sec is  the objective that we care about. \n",
    "# token_per_sec\n",
    "# how much of tokens of data are we training on and what is the throughput of \n",
    "# tokens that we're getting in our optimization \n",
    "\n",
    "# Andrej process : 16322 token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the pytorch documentation for this\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# after setting this all the matmul inside the linear layer should utilize the \n",
    "# tensor core with TF32 precision. \n",
    "\n",
    "# step 49, loss: 5.917618274688721, dt: 405.04ms, tok/sec: 20225.006897509764\n",
    "\n",
    "# we got 20225 which was 15115 \n",
    "# but andrej got around 3x more token (49k per second) instead of 8x \n",
    "# reason: a lot of these computation are memory bound. All the number are still float32 \n",
    "# they are being shipped all over the system and costing us way too much time to shuffle \n",
    "# around the data. So even thought we have made the multiply much faster, we are memory \n",
    "# bound and we are not getting the full benefit\n",
    "\n",
    "# try updating the cuda version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "FLOAT16  (All of this we are reading from Nvidia A100 GPU)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/\n",
    "FIG 7 \n",
    "\n",
    "- Maitaining 16bits per float\n",
    "- exponent part of the float representation: sets the range that you can represent of your numbers and the Precision is how much precision you have for your numbers. \n",
    "- so, the FP16 can't represent the full range of FP32 it has a reduced range and that's where you start to run into issue because now you have these gradient scalers.\n",
    "- So FP16 came in volta series before amphere and everyone in the AI space started to train FP16 but they had you use all these gradient scaling operations which are annoying and they additional source of state of complexity.And, the reason for that \n",
    "exponent range is reduced in FP16. \n",
    "- And then they came out with BF16 in the Amphere and they made it much simplier because we are just truncating mantissa and we have exact same range and we don't need the gradeint scaler. \n",
    "- Using BF16 might be impacting the number that we see our pytorch code. \n",
    "- Look at https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html\n",
    "- Ignore everything and only look at torch.autocast \n",
    "- if you look at document of torch.autocat it says that \n",
    "    - you only need to run your model and loss under autocast context manager\n",
    "    - see for context  https://pytorch.org/docs/stable/amp.html#torch.autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    #     logits, loss = model(x, y)  \n",
    "\n",
    "# this is only change we have to do in our code. \n",
    "# In contrast with FP32 this is going to impact our tensors \n",
    "\n",
    "# if you look at the logits.dtype >> torch.bfloat16 \n",
    "# mode.transformer.wte.weight.dtype >> torch.float32\n",
    "\n",
    "# so our parameters are still float32 but the activation (logits) are bfloat16\n",
    "# so this is why it's called a Mixed-precision. \n",
    "\n",
    "# Somethings in pytorch are convert to lower precision and somethings are kept to float32\n",
    "\n",
    "# and what gets converted is not very clear in the pytorch documentation. \n",
    "\n",
    "# so with BFLOAT16 are are losing the precision but it's a good trade as we can train faster. \n",
    "# since, we can train faster we can train our model for more iteration and over come that trade off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Mixed precision \n",
    "# ( DataLoaderLite(B=8, T=1024)\n",
    "# step 49, loss: 5.99915075302124, dt: 302.22ms, tok/sec: 27106.457056528754  (19 Gb memory on GPU)\n",
    "# DataLoaderLite(B=16, T=1024)\n",
    "# torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.54 GiB. GPU 0 has a total capacity of 23.70 GiB of which 1.26 GiB is free. Including non-PyTorch memory, this process has 22.19 GiB memory in use. Of the allocated memory 21.03 GiB is allocated by PyTorch, and 26.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
    "\n",
    "\n",
    "# andrej stats \n",
    "# step 49, loss: 6.06, dt: 199.64ms, tok/sec: 54678.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# torch.complie\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is bascially a complier for Neural Networks. \n",
    "- Like GCC for C/C++ code \n",
    "- https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use it just with one line of code \n",
    "# torch.compile(model)\n",
    "\n",
    "# this line of code will cost you compliation time. \n",
    "# but it will make your code run a lot faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Using torch.compile**\n",
    "# have to udpate the nvidia drivers\n",
    "# step 49, loss: 5.977720737457275, dt: 180.91ms, tok/sec: 45281.44075149841\n",
    "\n",
    "\n",
    "# Andrej stats \n",
    "# step 49, loss: 6.05, dt: 129.66, tok/sec: 126364.38\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Unless, you are debugging adn  you want your code to run really fast. \n",
    "- you should always use torch.compile(model)\n",
    "- Speedup mainly comes from reducing Python overhead and GPU read/writes.\n",
    "\n",
    "\n",
    "**So what is happending under the hood**\n",
    "- In code of GPT2 class, all we have is algorithmic description of what we would like to happen in our network and torch complie will analize the entire thing.\n",
    "- And, with the benefit of knowing, exactly what doing to happen it doesn't have to run a egear mode, it doesn't have to go like layer by layer and it optimize the process. \n",
    "- it will take out the Python interpreter from the forward pass entirely and will complie the entire NN as single object with no python interpreter involved. \n",
    "\n",
    "\n",
    "**Understanding GPU read/Writes**\n",
    "\n",
    "- How a this operation works? \n",
    "\n",
    "say you have (x ** 3) ** 0.5 where x is tensor of size (10, 20)\n",
    "\n",
    "- the tensor will be stored in GPU memeory and when you execute this operation. This tensor will travel to GPU Cores, performes the operation and the result will be saved back to the GPU memory. \n",
    "- During, this travel of tensor backandforth from memory to core the GPU's memory bandwith comes into picture. \n",
    "- not for ** 0.5  this operation GPu will repeat the process of moving the tensor from memory to cores, do the operation and write back to it. \n",
    "- with torch.complie() it already know how to what are operation you are going to perform on the tensor and can load the tensor once, do all the operation and write back the result. This saved al the round trips. \n",
    "- This is one of example which is called as kernel fusion. \n",
    "- Look at the video at 1:55:00 for more deeper understanding of GPU architure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Flash attention\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It come sfrom the papaer: Flash attention: Fast and memory-efficient exact attention with IO-Awareness\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flash attention implement this 4 lines very very quickly\n",
    "\n",
    "    # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "    # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "    # att = F.softmax(att, dim=-1)\n",
    "    # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "# How does it do that??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Flash attention is a kenerl Fusion operation \n",
    "- Look at the Fig1 from the paper, the left side of the picture is equal to the above 4 lines that we have (except for the dropout, as we are not using it).\n",
    "- Instead of using these 4 lines we are fusing them into a single fused kernel of flash attention. \n",
    "- It's a kernel fusion algorigthm but it's a kernel fusion that torch compile can't find and the reason it can't find it because it required a algorithmic re-write of how attenstion is actually implemented here in this case. \n",
    "- If you count the num of flops, the flash attention does more flops than the attention is used using those 4 lines of code. But, it is significianlty faster, infact 7.6 times faster. \n",
    "- That is because, it is very mindful of the memory hierarchy. It is very mindful of what is in HBM (High Bandwidth Memory), what's in the shared memory, and it's very careful about how to orchestrates the computation such that we have fewer read and writes to HBM and so even though we are doing more flops, the expensive part is they load and store into hbm and that's what they avoid. \n",
    "- Flash attention designed that self-attention (att matix) never get materialized at any point and never gets read or written to the HBM.\n",
    "- And, the way that this is acheivided. The fundamental algorithmic rewrite here relies on this online softmax trick.\n",
    "- Online Softmax trick (come from a previous paper). It shows how you can incrementally evaluate a softmax without having to sort of realize all of the inputs of the softmax to do the normalization.And, you do that by having these intermediate variable m and l (looking at equation from the paper 3.1 section)\n",
    "and there is a update to them that allows you to evaluate the softmax in online manner. \n",
    "- Recently; Flash Attention-2 : Faster attention with better parallelism and work partitioning that has additional gains to how it calculates flash attention and the original paper that this is based on basically \"Online normalizer calculation for softmax\" by Nvidia. \n",
    "- Key take away from these papers\n",
    "    - Be aware of memory hierarchy. \n",
    "    - The fact that flops don't matter and the entire memory access pattern matters.\n",
    "    - torch complie is amazing but there are many optimization still avaiable to us that potentially torch complie can't find. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after implementing Flash-attention. \n",
    "# step 49, loss: 5.978326797485352, dt: 147.64ms, tok/sec: 55486.14267927764\n",
    "\n",
    "# andrej stats \n",
    "# step 49, loss: 6.05, dt: 96.64ms, tok/sec: 169723.40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Nice/Ugly numbers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nice numbers: which have a lot of power of 2 \n",
    "- Ugly numbers : 13, 17 , 7 \n",
    "- You always want to use Nice numbers in all of your code that deals with NN or Cuda. Because everything in Cuda works in like power of 2 and a lot of kernels are written in power of 2 and there are a lot of blocks of size 16, 64 and so on. \n",
    "- So, go over the entire code and look for ugly numbers \n",
    "- vocab_size = 50257 is very ugly number.\n",
    "- How to fix this?\n",
    "- you basically, increase the number until it's nearest power of 2 that you like. \n",
    "- 50304 : as it can be divided by 8, 16, 32, 64, 128. This is like we are adding fake tokens and these tokens are never used. \n",
    "- If think about it by adding these fake token, at the last layer i.e the classifier what this is doing is that we are predicting additional dimensions at the classifier now and we are predicting probalilites for the tokens that of course never be present in the training set. Therefore, the network has to learn that these probabalities have to driven to zero. So, the logits that network produces have to drive those dimensions of output to -inf. \n",
    "- In our shakespeare example, anyway we just have few thousand tokens and the rest of them is just pushed down to zero. we are just adding few more. \n",
    "- We have to make sure that this doesn't break anything.\n",
    "- In conclusion, if you think about it we are adding computation but we are running faster and the reason for this as we have discuss above. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
