{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Paper: https://arxiv.org/pdf/2005.14165\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section3: Hyperparameters, AdamW, gradient clipping\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Up until now, we have made all the changes in our NN to beter utilize our GPU. Now, we are going to make algorithm changes and improvement of actual optimization itself. \n",
    "- And, to do this we like to follow the hyper-parameters that are mentioned in GPT2 or GPT3 paper. There is not much to look in the GPT2 paper or the code that is relesed by GPT2. \n",
    "- So, we will look the appendix of GPT3 paper. \n",
    "    - B. Details of model training. \n",
    "\n",
    "**Change1**\n",
    "- we change our code as per hyperparameter given in the paper. \n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    \n",
    "    will changeed to \n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change2: Gradient Norm Clipping** : \n",
    "- we clip the global norm of the gradient at 1.0. \n",
    "- This is referening to once we have calculated the gradient after loss.backward()\n",
    "- we have gradients at all the parameter tensors and what people like to do is basically clip them to have some kind of maximum norm \n",
    "\n",
    "\n",
    "norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "- norm: is the norm of the gradient. \n",
    "- what this function is doing is calculating the global norm of the parameters. so every single gradient on all the parameters you square it, add it all up and you take a big square root of that and that's the norm of parameter vector basically. It's bascially the length of it. And, we are basically making sure the length of that vector is not more than 1.0. we are going to clip it. \n",
    "- And, the reason people like to use is that sometimes you can get unlucky during the optimization may be it's a bad data batch or something like that and if you get very unlucky in a batch you might get really high loss and really high loss can lead to really light gradient and this could basically shock your model and shock the optimizations. \n",
    "- So, people like to use gradient norm clipping to prevent the model from getting too big of a shock in term of gradient magnitude and it's upper bounded in this way. \n",
    "- It's like hacky solution but people still do it quite frequently. \n",
    "- if the value of norm is well behaved then things are good and if it's climbing then things are bad and they are destabilizing during training and sometimes you can get a spike in the norm and means there is some kind of issue or an instability \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 0 | loss: 10.933823585510254 | norm: 29.4195 | dt: 13843.83ms | tok/sec: 591.7439477574981\n",
    "# step 49 | loss: 5.918039321899414 | norm: 1.2791 | dt: 145.53ms | tok/sec: 56290.43590688744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "**Change3: Learning rate Scheduler**\n",
    "\n",
    "**Cosine Decay with warmup**\n",
    "\n",
    "\n",
    "- They don't use a fixed learning rate like we are using up until now. \n",
    "- They are using a cosine decay for learning rate \n",
    "- And, they way this looks (look at the graph at https://scorrea92.medium.com/cosine-learning-rate-decay-e8b50aa455b)\n",
    "- Learning rate starts at zero, linearly ramps up for some amount of time and comes down with this cosine sort of form to a minimum lr that is up to you. \n",
    "- In paper, they said \"we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260\n",
    "billion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the first 375\n",
    "million tokens\"\n",
    " - check the commit for code changes. \n",
    " - There is cosine learning rate scheduler in pytorch as well but like the to write it's own code for this so that he fully understand it and it's just a couple of lines of code. \n",
    " - so we learly warm up to max lr rate and then we start to decay it. \n",
    " - One thing we are not following what they did is that, there training horizon is 300 Billion tokens and they come down to initial lr at 260 billion and they train after 260 at 10%. Bascially, there decay time is less than the max steps time whereas for us it's exactly equal. \n",
    "\n",
    "- What learning rate you use is totally up to you. Cosine lr has been popularized by GPT2 and GPT3 and this is active area of research.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using device: cuda\n",
    "# loaded 338025 tokens\n",
    "# 1 epoch = 41 batches\n",
    "# step    0 | loss: 10.933824 | lr 6.0000e-05 | norm: 29.4194 | dt: 13769564.39ms | tok/sec: 594.94\n",
    "# step    1 | loss: 9.647558 | lr 1.2000e-04 | norm: 9.8480 | dt: 143231.39ms | tok/sec: 57194.17\n",
    "# step    2 | loss: 8.992529 | lr 1.8000e-04 | norm: 5.8678 | dt: 142774.11ms | tok/sec: 57377.35\n",
    "# step    3 | loss: 9.544296 | lr 2.4000e-04 | norm: 7.4843 | dt: 142538.31ms | tok/sec: 57472.27\n",
    "# step    4 | loss: 8.959856 | lr 3.0000e-04 | norm: 4.2276 | dt: 143009.90ms | tok/sec: 57282.75\n",
    "# step    5 | loss: 8.671993 | lr 3.6000e-04 | norm: 3.0261 | dt: 143142.22ms | tok/sec: 57229.79\n",
    "# step    6 | loss: 8.600800 | lr 4.2000e-04 | norm: 3.4741 | dt: 143462.66ms | tok/sec: 57101.97\n",
    "# step    7 | loss: 8.174708 | lr 4.8000e-04 | norm: 2.5680 | dt: 143914.94ms | tok/sec: 56922.51\n",
    "# step    8 | loss: 7.772866 | lr 5.4000e-04 | norm: 2.7331 | dt: 149877.79ms | tok/sec: 54657.87\n",
    "# step    9 | loss: 7.570984 | lr 6.0000e-04 | norm: 2.2930 | dt: 143419.27ms | tok/sec: 57119.24\n",
    "# step   10 | loss: 7.380367 | lr 6.0000e-04 | norm: 2.0155 | dt: 143720.15ms | tok/sec: 56999.66\n",
    "# step   11 | loss: 7.104572 | lr 5.9917e-04 | norm: 1.5674 | dt: 143507.00ms | tok/sec: 57084.32\n",
    "# step   12 | loss: 6.973597 | lr 5.9668e-04 | norm: 1.1982 | dt: 143562.56ms | tok/sec: 57062.23\n",
    "# step   13 | loss: 6.736616 | lr 5.9254e-04 | norm: 1.2940 | dt: 143640.52ms | tok/sec: 57031.26\n",
    "# step   14 | loss: 6.641829 | lr 5.8679e-04 | norm: 0.8845 | dt: 143579.24ms | tok/sec: 57055.60\n",
    "# step   15 | loss: 6.466947 | lr 5.7945e-04 | norm: 2.0647 | dt: 143687.96ms | tok/sec: 57012.43\n",
    "# step   16 | loss: 6.580324 | lr 5.7057e-04 | norm: 1.2491 | dt: 146642.45ms | tok/sec: 55863.77\n",
    "# step   17 | loss: 6.646888 | lr 5.6021e-04 | norm: 1.4116 | dt: 146014.45ms | tok/sec: 56104.04\n",
    "# step   18 | loss: 6.574047 | lr 5.4843e-04 | norm: 1.4114 | dt: 144019.37ms | tok/sec: 56881.24\n",
    "# step   19 | loss: 6.360060 | lr 5.3531e-04 | norm: 1.3833 | dt: 145360.95ms | tok/sec: 56356.26\n",
    "# step   20 | loss: 6.478446 | lr 5.2092e-04 | norm: 1.7494 | dt: 147783.52ms | tok/sec: 55432.43\n",
    "# step   21 | loss: 6.282598 | lr 5.0535e-04 | norm: 1.5478 | dt: 144555.81ms | tok/sec: 56670.16\n",
    "# step   22 | loss: 6.409733 | lr 4.8870e-04 | norm: 1.1385 | dt: 148765.33ms | tok/sec: 55066.60\n",
    "# step   23 | loss: 6.232314 | lr 4.7107e-04 | norm: 1.1030 | dt: 146510.36ms | tok/sec: 55914.13\n",
    "# step   24 | loss: 6.251258 | lr 4.5258e-04 | norm: 1.2467 | dt: 144739.15ms | tok/sec: 56598.37\n",
    "# step   25 | loss: 6.281051 | lr 4.3332e-04 | norm: 0.9518 | dt: 143994.33ms | tok/sec: 56891.13\n",
    "# step   26 | loss: 6.606372 | lr 4.1343e-04 | norm: 1.1651 | dt: 145787.72ms | tok/sec: 56191.29\n",
    "# step   27 | loss: 6.461005 | lr 3.9303e-04 | norm: 1.2547 | dt: 148341.42ms | tok/sec: 55223.96\n",
    "# step   28 | loss: 6.730590 | lr 3.7224e-04 | norm: 1.1376 | dt: 144857.65ms | tok/sec: 56552.07\n",
    "# step   29 | loss: 6.465397 | lr 3.5118e-04 | norm: 1.0399 | dt: 144737.48ms | tok/sec: 56599.02\n",
    "# step   30 | loss: 6.425256 | lr 3.3000e-04 | norm: 0.9473 | dt: 145284.18ms | tok/sec: 56386.04\n",
    "# step   31 | loss: 6.396163 | lr 3.0882e-04 | norm: 1.0962 | dt: 144153.36ms | tok/sec: 56828.37\n",
    "# step   32 | loss: 6.242393 | lr 2.8776e-04 | norm: 1.1938 | dt: 144170.52ms | tok/sec: 56821.60\n",
    "# step   33 | loss: 6.582739 | lr 2.6697e-04 | norm: 1.9381 | dt: 144218.21ms | tok/sec: 56802.81\n",
    "# step   34 | loss: 6.419995 | lr 2.4657e-04 | norm: 1.4231 | dt: 145292.04ms | tok/sec: 56382.99\n",
    "# step   35 | loss: 6.328524 | lr 2.2668e-04 | norm: 0.9130 | dt: 144602.54ms | tok/sec: 56651.84\n",
    "# step   36 | loss: 6.373841 | lr 2.0742e-04 | norm: 1.0457 | dt: 146884.20ms | tok/sec: 55771.82\n",
    "# step   37 | loss: 6.393122 | lr 1.8893e-04 | norm: 0.9558 | dt: 146669.39ms | tok/sec: 55853.51\n",
    "# step   38 | loss: 6.166686 | lr 1.7130e-04 | norm: 0.9296 | dt: 144530.30ms | tok/sec: 56680.16\n",
    "# step   39 | loss: 6.256642 | lr 1.5465e-04 | norm: 1.0896 | dt: 145495.41ms | tok/sec: 56304.18\n",
    "# step   40 | loss: 6.440091 | lr 1.3908e-04 | norm: 1.1819 | dt: 144564.63ms | tok/sec: 56666.70\n",
    "# step   41 | loss: 6.261865 | lr 1.2469e-04 | norm: 0.9343 | dt: 144588.95ms | tok/sec: 56657.17\n",
    "# step   42 | loss: 6.312222 | lr 1.1157e-04 | norm: 1.1466 | dt: 146482.94ms | tok/sec: 55924.60\n",
    "# step   43 | loss: 6.033404 | lr 9.9787e-05 | norm: 1.2428 | dt: 145900.96ms | tok/sec: 56147.68\n",
    "# step   44 | loss: 5.987703 | lr 8.9428e-05 | norm: 0.9310 | dt: 144826.17ms | tok/sec: 56564.36\n",
    "# step   45 | loss: 6.059750 | lr 8.0553e-05 | norm: 0.8940 | dt: 144172.91ms | tok/sec: 56820.66\n",
    "# step   46 | loss: 6.136729 | lr 7.3215e-05 | norm: 0.7327 | dt: 145359.75ms | tok/sec: 56356.73\n",
    "# step   47 | loss: 6.001136 | lr 6.7460e-05 | norm: 1.1086 | dt: 144611.84ms | tok/sec: 56648.20\n",
    "# step   48 | loss: 5.896204 | lr 6.3324e-05 | norm: 0.8215 | dt: 146751.64ms | tok/sec: 55822.20\n",
    "# step   49 | loss: 5.787439 | lr 6.0832e-05 | norm: 0.8931 | dt: 145449.88ms | tok/sec: 56321.81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Next: Gradual Batch size increase\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the paper \"We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over\n",
    "the first 4-12 billion tokens of training, depending on the model size\"\n",
    "- we are going to skip this. \n",
    "- Reason: It complicates a lot of the arithmetic because you are changing the number of tokens that you are processing at every single step of the optimization and Andrej like to keep that math very very simple. \n",
    "- Also, this is not like a alogorithm improvement. It's more of system and speed improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "From paper: \"Data are sampled without replacement during\n",
    "training (until an epoch boundary is reached) to minimize overfitting. \"\n",
    "\n",
    "- we are already doing that, in our dataloader once the data has been draw it's is not eligible to drawn again until the next epoch. \n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### Next: From Paper: \"All models use weight decay of 0.1 to provide a small amount of regularization\"\n",
    "\n",
    "\n",
    "code changes from \n",
    "- addded a new function to configure_optimizers to class GPT. \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "\n",
    "to \n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have create a new function in the GPT class. \n",
    "- It's common to not weight decay biases or any other sort of 1-dim tensors. like layernorm, weight decay, baises. \n",
    "- You want to weight decay the weights that participate in matric multiplications and embeddings.\n",
    "- why does it make sense to decay the weights?\n",
    "- You can think of this as an regularization. Because when you pulling down all the weights.You are forcing the optimization to use more of weights, and you are not allowing any one of the weight individually to be way too large. \n",
    "- You are forcing the keyword to kinda like distributing the work across more channels because they are like the pull of gravity on the weights themselves.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next: Fused AdamW \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'fused' in inspect.signature(torch.optim.AdamW).parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As previous version of AdamW didn't not had fused implementation therefore we are gurading it. \n",
    "- What is fused meaning here? \n",
    "- So, instead of iterating over all the parameters tensors in for loop and updating them. As, that would launch a lot of kernels.\n",
    "- Fused means that all those kernels are fused into single kernel, you got rid of the a lot of overhead and you single time on all the parameters call a kernel that updates them. \n",
    "- Basically, it like the kernel fusion for AdamW optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before \n",
    "# step   30 | loss: 6.408119 | lr 3.3000e-04 | norm: 0.7270 | dt: 146337.27ms | tok/sec: 55980.27\n",
    "\n",
    "\n",
    "#using device: cuda\n",
    "# loaded 338025 tokens\n",
    "# 1 epoch = 41 batches\n",
    "# num decayed parameter tensors: 50, with 124,354,560 parameters\n",
    "# num non-decayed parameter tensors: 98, with 121,344 parameters\n",
    "# using fused AdamW: True\n",
    "# step   49 | loss: 5.786849 | lr 6.0832e-05 | norm: 0.9179 | dt: 142815.59ms | tok/sec: 57360.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**\n",
    "- The Relationship b/w weight decay, learning rate, batch size and the adam parameter beta1, beta2 and the epslion. These are very complicated mathematical relationship in the optimization literature.\n",
    "\n",
    "---- \n",
    "\n",
    "- One probelem for us that we can't use the same batch size used to train the GPT model from the paper. (Looking the Table 2.1 from GPT 3)\n",
    "- We can't just use 0.5M as our batch_size (B) and 0.5M is the batch size in the number of tokens. So, I have to pass B=488 to match this. \n",
    "- But, we can't do because this dataset will not fill in our GPU. But we still want to use this batch_size as it is corelated with all the other optimization hyperparameters and lr. \n",
    "- But, now the problem is how can we use 0.5M as our batch size on our small GPU. \n",
    "- Now for that we have use \"Gradient accumulation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488.28125"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5e6/1024 # 5M / number of channels (as each one of our row is of size 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Gradient Accumulation\n",
    "\n",
    "---- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It will allow use to simulate in a serial way any arbitary batch_size that we set. \n",
    "- So, we can do batch size of 0.5M. we just have to run it longer and we have to process multiple sequences and add up all the gradients from them to simlute a batch size 0.5M. \n",
    "- we still have B, T matix that go into the transformer and do forward and backward but we are not going to do an update. \n",
    "- We are going to do many forwards and backwards and those gradients are all going to += on the parameter gradients. They all gonna add up. \n",
    "- So we are going to do forward and backward 'grad_accum_steps' number of times and then we are going to a single update once all of the is accumulated. \n",
    "- So our micro batch size is now controlling how many tokens, how many rows we are processing in a single go over a foward and backward. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64.0, 32.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change for gradient accumulation \n",
    "524288 / (8 * 1024), 524288 / (16 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens andrej doing per forward backward \n",
    "16 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also, andrej is doing 32 fordward and backward and then \n",
    "# single update.\n",
    "524288 / (16 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt : is time for single forward and backward \n",
    "\n",
    "# forward and backward pass \n",
    "    # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    #     logits, loss = model(x, y)  \n",
    "    # loss.backward()\n",
    "\n",
    "# this part have to do 32 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # for micro_step in range(grad_accum_steps):\n",
    "    #     x, y = train_loader.next_batch()\n",
    "    #     x, y = x.to(device), y.to(device)\n",
    "    #     with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    #         logits, loss = model(x, y)  \n",
    "    #     loss.backward()\n",
    "\n",
    "# now this loop right here have a issue. We will look at the toy example to understand thisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047,\n",
      "         -0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688,  0.7624],\n",
      "        [ 1.6423, -0.1596, -0.4974,  0.4396, -0.7581,  1.0783,  0.8008,  1.6806,\n",
      "          1.2791,  1.2964,  0.6105,  1.3347, -0.2316,  0.0418, -0.2516,  0.8599],\n",
      "        [-1.3847, -0.8712, -0.2234,  1.7174,  0.3189, -0.4245,  0.3057, -0.7746,\n",
      "         -1.5576,  0.9956, -0.8798, -0.6011, -1.2742,  2.1228, -1.2347, -0.4879],\n",
      "        [-0.9138, -0.6581,  0.0780,  0.5258, -0.4880,  1.1914, -0.8140, -0.7360,\n",
      "         -1.4032,  0.0360, -0.0635,  0.6756, -0.0978,  1.8446, -1.1845,  1.3835]])\n",
      "---------------\n",
      "y tensor([[ 0.1331],\n",
      "        [ 0.8640],\n",
      "        [-1.0157],\n",
      "        [-0.8887]])\n",
      "---------------\n",
      "---------------\n",
      "yhat tensor([[-0.6010],\n",
      "        [-0.0064],\n",
      "        [-0.2213],\n",
      "        [-0.0824]], grad_fn=<AddmmBackward0>)\n",
      "---------------\n",
      "tensor([-0.0150,  0.0011,  0.0042, -0.0040,  0.0059, -0.0080, -0.0078, -0.0138,\n",
      "        -0.0103, -0.0134])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# case 1\n",
    "# super simple little MLP\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(16, 32),\n",
    "    torch.nn.GELU(),\n",
    "    torch.nn.Linear(32, 1)\n",
    ")\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "x = torch.randn(4, 16)\n",
    "y = torch.randn(4, 1)\n",
    "\n",
    "print('x', x)\n",
    "print(\"-\"*15)\n",
    "print('y', y)\n",
    "print(\"-\"*15)\n",
    "\n",
    "net.zero_grad()\n",
    "yhat = net(x)\n",
    "print(\"-\"*15)\n",
    "print('yhat', yhat)\n",
    "\n",
    "\n",
    "loss = torch.nn.functional.mse_loss(yhat, y)\n",
    "loss.backward()\n",
    "print(\"-\"*15)\n",
    "print(net[0].weight.grad.view(-1)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the above simple NN, we are just doing the simple regression with mean squared loss over those 4 example in y tensor. \n",
    "- If you look the documentation of mse_loss(.... reduction='mean'). and the value of reduction is mean by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the loss objective here(due to reduction='mean')\n",
    "L = 1/4 * [\n",
    "    # here we are calculting the squared error for \n",
    "    # all the 4 examples individually, summing them up \n",
    "    # and normalizing by (1/4) to make this mean-squared error \n",
    "    (y[0] - yhat[0])**2 + \n",
    "    (y[1] - yhat[1])**2 + \n",
    "    (y[2] - yhat[2])**2 + \n",
    "    (y[3] - yhat[3])**2 \n",
    "]\n",
    "# NOTE: 1/4! \n",
    "# The 1/4 is the average, because there are 4 independent examples here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0598,  0.0042,  0.0167, -0.0161,  0.0235, -0.0320, -0.0311, -0.0550,\n",
      "        -0.0410, -0.0536])\n"
     ]
    }
   ],
   "source": [
    "# case 2\n",
    "# now if we look at the Gradient Accumulation version of our NN.\n",
    "# here we have grad_accum_steps of 4 \n",
    "\n",
    "net.zero_grad() # this is reset the gradient \n",
    "for i in range(4):\n",
    "    yhat = net(x[i])\n",
    "    loss = torch.nn.functional.mse_loss(yhat, y[i])\n",
    "    loss.backward()\n",
    "print(net[0].weight.grad.view(-1)[:10])\n",
    "\n",
    "\n",
    "# in the above loop we are we are evaluating all the example individually \n",
    "# and then caling loss.backward() on them each time and once the loop is \n",
    "# finished then we are looking at the gradient that we have achieved from that\n",
    "# now, if you look the gradient, will not match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in cadse 1: we did the single batch of 4 and \n",
    "# in the case 2 we did four gradient accumulation steps of batch_size one\n",
    "# and gradient are not the same \n",
    "# And, the reason that they are not the same because this mean-squared error (the 1/4) gets lost in case 2\n",
    "\n",
    "# because the loss objective of every one of the loops in case 2 \n",
    "from itertools import tee\n",
    "\n",
    "\n",
    "L0 = (y[0] - yhat[0])**2 \n",
    "L1 = (y[1] - yhat[1])**2 \n",
    "L2 = (y[2] - yhat[2])**2 \n",
    "L3 = (y[3] - yhat[3])**2 \n",
    "\n",
    "\n",
    "# when we did loss.backward() we are accumulating the gradient and \n",
    "# accumulation in gradient is equivalent to the SUM in loss\n",
    "# so our loss here is as below \n",
    "L = L0 + L1 + L2 + L3\n",
    "\n",
    "# without the factor of (1/4)\n",
    "# NOTE : so we are missing the normalizer and therefore our gradients are off. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One way of fixing this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0150,  0.0011,  0.0042, -0.0040,  0.0059, -0.0080, -0.0078, -0.0138,\n",
      "        -0.0103, -0.0134])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad() # this is reset the gradient \n",
    "for i in range(4):\n",
    "    yhat = net(x[i])\n",
    "    loss = torch.nn.functional.mse_loss(yhat, y[i])\n",
    "    loss = loss / 4\n",
    "    loss.backward()\n",
    "print(net[0].weight.grad.view(-1)[:10])\n",
    "\n",
    "# internally we are doing is NOTE: just adding (1/4) \n",
    "# L0 = 1/4 (y[0] - yhat[0])**2 \n",
    "# L1 = 1/4 (y[1] - yhat[1])**2 \n",
    "# L2 = 1/4 (y[2] - yhat[2])**2 \n",
    "# L3 = 1/4 (y[3] - yhat[3])**2 \n",
    "\n",
    "# Now, if we run this then our weights should be equal to what we got in the case1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- Now, going back to our code. We are using F.cross_entropy() and the default value of reduction here is also mean loss of all the B*T elements.\n",
    "- so we need to add this line in our loop: \n",
    "        loss = loss/grad_accum_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
