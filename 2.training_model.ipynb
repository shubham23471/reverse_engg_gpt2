{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# simplest data to work with : tiny shakespare \n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "data = text[:1000]\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198, 198, 5962, 22307, 25, 198, 1639, 389, 477, 12939, 2138, 284, 4656, 621, 284, 1145, 680, 30, 198, 198, 3237, 25, 198, 4965, 5634, 13, 12939, 13, 198, 198, 5962, 22307, 25, 198, 5962, 11, 345, 760, 327, 1872, 385, 1526, 28599, 318, 4039, 4472, 284, 262, 661, 13, 198, 198, 3237, 25, 198, 1135, 760, 470, 11, 356, 760, 470, 13, 198, 198, 5962, 22307, 25, 198, 5756, 514, 1494, 683, 11, 290, 356, 1183, 423, 11676, 379, 674, 898, 2756, 13, 198, 3792, 470, 257, 15593, 30, 198, 198, 3237, 25, 198, 2949, 517, 3375, 319, 470, 26, 1309, 340, 307, 1760, 25, 1497, 11, 1497, 0, 198, 198, 12211, 22307, 25, 198, 3198, 1573, 11, 922, 4290, 13, 198, 198, 5962, 22307, 25, 198, 1135, 389, 17830, 3595, 4290, 11, 262, 1458, 1173, 1547, 922, 13, 198, 2061, 4934, 969, 5036, 896, 319, 561, 26958, 514, 25, 611, 484, 198, 19188, 7800, 514, 475, 262, 48713, 414, 11, 981, 340, 547, 198, 1929, 4316, 462, 11, 356, 1244, 4724, 484, 22598, 514, 31533, 306, 26, 198, 4360, 484, 892, 356, 389, 1165, 13674, 25, 262, 10904, 1108, 326, 198, 2001, 42267, 514, 11, 262, 2134, 286, 674, 24672, 11, 318, 355, 281, 198, 24807, 284, 1948, 786, 511, 20038, 26, 674, 198, 82, 13712, 590, 318, 257, 4461, 284, 606, 3914, 514, 15827, 428, 351, 198, 454, 279, 7938, 11, 304, 260, 356, 1716, 374, 1124, 25, 329, 262, 11858, 760, 314, 198, 47350, 428, 287, 16460, 329, 8509, 11, 407, 287, 24613, 329, 15827, 13, 628]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(data)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
       "        [ 5120,   597,  2252,    11,  3285,   502],\n",
       "        [ 2740,    13,   198,   198,  3237,    25],\n",
       "        [  198,  5248,   461,    11,  2740,    13]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we want to take these tokens and forward them in our GPT model \n",
    "# to do that we have reshape our long list of token sequence into a (B, T) tensor \n",
    "# before that tha shape our forward function takes\n",
    "# take an example of 24 tokens to understand this\n",
    "\n",
    "import torch\n",
    "buf = torch.tensor(tokens[:24])\n",
    "x = buf.view(4, 6)\n",
    "x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, that's our input to the transformer. Now, we also need labels to calucate the loss function. \n",
    "- For this we can write some code in the forward pass of the GPT model, as we know the next sequence is just the right of us. But the problem with that approch is that the for the token 13 (the last toke) we don't have the label information. \n",
    "- So Andrej fav way to do this, is that we will create the another tensor label of exact same size as x but it contains the labels at every position. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
       "         [ 5120,   597,  2252,    11,  3285,   502],\n",
       "         [ 2740,    13,   198,   198,  3237,    25],\n",
       "         [  198,  5248,   461,    11,  2740,    13]]),\n",
       " tensor([[22307,    25,   198,  8421,   356,  5120],\n",
       "         [  597,  2252,    11,  3285,   502,  2740],\n",
       "         [   13,   198,   198,  3237,    25,   198],\n",
       "         [ 5248,   461,    11,  2740,    13,   198]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so, in the buf we get the last token + 1 and then index our x and y \n",
    "buf = torch.tensor(tokens[:24 + 1 ])\n",
    "x = buf[:-1].view(4, 6) \n",
    "y = buf[1:].view(4, 6) \n",
    "x, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we go back to our script and create a very simpel dataloader object to load these tokens, feed them to transformer and calculate the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- \n",
    "\n",
    "Now, we want to calculate the loss function and to do so we have to modify the forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using device: cuda\n",
    "Only loading first 1k charchters as model input\n",
    "logits shape torch.Size([4, 32, 50257])\n",
    "loss tensor(11.0371, device='cuda:0', grad_fn=<NllLossBackward0>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9897725689953638e-05, -10.82490511970208)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "1/ 50257, -math.log(50257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"step {i}, loss: {loss.item()}\")\n",
    "\n",
    "# when we use this we are overfitting on the one batch x that we are passing again and again.\n",
    "# lr = 3e-4 : is like good standard learning rate for early debugging stage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we don't want to overfit our model on that one batch. We want to create the a data loader which makes sure that we are getting a fresh batch. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num of batches in a single epoch of iterating over a single dataset\n",
    "- So 1 epoch : how many unique batches do we output/iterate over before we loop back begining of the document and start reading it again. \n",
    "1000/ (4 * 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 8714)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000 // (4 * 32), 1115394 // (4 * 32) # 1115394: num of charachter but we have to use the num \n",
    "# of tokens\n",
    "# So "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 epoch => num of tokens // (B, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiktokener have roughly about 1 to 3 compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using device: cuda\n",
    "# loaded 338025 tokens\n",
    "# 1 epoch = 2640 batches\n",
    "# step 0, loss: 11.08803939819336\n",
    "# step 1, loss: 9.717626571655273\n",
    "# step 2, loss: 8.71585750579834\n",
    "# step 3, loss: 8.921952247619629\n",
    "# step 4, loss: 8.44729995727539\n",
    "# step 5, loss: 8.131488800048828\n",
    "# step 6, loss: 9.005425453186035\n",
    "# step 7, loss: 8.671504020690918\n",
    "# step 8, loss: 8.075515747070312\n",
    "# step 9, loss: 7.828042984008789\n",
    "# step 10, loss: 8.26616096496582\n",
    "# step 11, loss: 7.185250282287598\n",
    "# step 12, loss: 7.63140869140625\n",
    "# step 13, loss: 7.262790679931641\n",
    "# step 14, loss: 7.483865261077881\n",
    "# step 15, loss: 7.127208232879639\n",
    "# step 16, loss: 7.217015743255615\n",
    "# step 17, loss: 8.252432823181152\n",
    "# step 18, loss: 7.073562145233154\n",
    "# step 19, loss: 7.671370029449463\n",
    "# step 20, loss: 7.384253025054932\n",
    "# step 21, loss: 7.580746173858643\n",
    "# step 22, loss: 6.214944839477539\n",
    "# step 23, loss: 6.54738712310791\n",
    "# step 24, loss: 6.535855293273926\n",
    "# step 25, loss: 6.208362579345703\n",
    "# step 26, loss: 6.298552989959717\n",
    "# step 27, loss: 7.4446821212768555\n",
    "# step 28, loss: 6.870301723480225\n",
    "# step 29, loss: 6.647202968597412\n",
    "# step 30, loss: 6.902033805847168\n",
    "# step 31, loss: 6.967312335968018\n",
    "# step 32, loss: 6.967711448669434\n",
    "# step 33, loss: 6.67465353012085\n",
    "# step 34, loss: 7.907723426818848\n",
    "# step 35, loss: 7.669620513916016\n",
    "# step 36, loss: 7.351398944854736\n",
    "# step 37, loss: 7.470437049865723\n",
    "# step 38, loss: 7.569716453552246\n",
    "# step 39, loss: 7.141706943511963\n",
    "# step 40, loss: 7.261336803436279\n",
    "# step 41, loss: 6.442892551422119\n",
    "# step 42, loss: 6.642857551574707\n",
    "# step 43, loss: 6.760526180267334\n",
    "# step 44, loss: 6.7352776527404785\n",
    "# step 45, loss: 6.635310173034668\n",
    "# step 46, loss: 5.62755012512207\n",
    "# step 47, loss: 5.949457168579102\n",
    "# step 48, loss: 6.788867473602295\n",
    "# step 49, loss: 6.460808753967285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bug while loading the weight from hugging face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/projects/pytorch-concepts/torchenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model_hf = GPT2LMHeadModel.from_pretrained('gpt2') \n",
    "sd_hf = model_hf.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 768])\n",
      "torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(sd_hf['lm_head.weight'].shape)\n",
    "print(sd_hf[\"transformer.wte.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.wte : is the token embedding at the bottom of the transformer\n",
    "# lm_head : is the language modeling head at the top of the transformer\n",
    "        #  this is taking the 768 channels of the transformer and up scale that to \n",
    "        #   50257 to get the logits for the next token\n",
    "\n",
    "# both of these are 2-dim tensor and there shape is identical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing there every single element\n",
    "(sd_hf['lm_head.weight'] == sd_hf['transformer.wte.weight']).all()\n",
    "# as you can see that all the elemets are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130897059688403\n",
      "130897059688403\n"
     ]
    }
   ],
   "source": [
    "# checking the data pointer \n",
    "print(sd_hf['lm_head.weight'].data_ptr())\n",
    "print(sd_hf['transformer.wte.weight'].data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see they have the same pointer to data as well. \n",
    "# so what's happending here is that they have same weight are sharing b/w these two\n",
    "\n",
    "# look the 3.4 Embedding and softmax section of attention all you need paper\n",
    "# The original ideas comes from the paper: Using output embedding to Language models\n",
    "\n",
    "# intiutive idea\n",
    "# You want these two matrices to behave similar in the following sense\n",
    "# if two tokens are very similar semantically like one of them is a lowercase and other \n",
    "# is all uppercase or the same token in a different langauage or something like that. \n",
    "\n",
    "# If you have similarity b/w two token presumbly you would expect that they are near by \n",
    "# in the token embedding space but in the exact same way you'd except that if you have two \n",
    "# tokens that are similar semantically you'd except them to get same probablities at the \n",
    "# output of the transfomer because they are semantically similar. \n",
    "\n",
    "# so both positions in the transfomers at the very bottom and at the top have this property \n",
    "# that similar tokens should have similar embeddings.\n",
    "\n",
    "# They also observed (in the paper) if you look at the output embeddings they also behaves \n",
    "# like word embeddings \n",
    "\n",
    "# so basicalyy wte is used twice \n",
    "# 1. at the bottom of the transformer \n",
    "# 2. at the top of the transformer\n",
    "# and In the backward pass we will get gradient contributions from both branches and \n",
    "# these gradients will add up on wte tensor\n",
    "# so we will get contribution from the classifier layer and also at the bottom of the \n",
    "# transfomer \n",
    "\n",
    "# other than this this also helps in computations. as you can these are lot of \n",
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38597376"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50257 * 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rought like 40M parameters are shared "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So that's the change we want to make in out code. is to share wte \n",
    "--- \n",
    "- after changing the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sing device: cuda\n",
    "# loaded 338025 tokens\n",
    "# 1 epoch = 2640 batches\n",
    "# step 0, loss: 10.969259262084961\n",
    "# step 1, loss: 9.712841033935547\n",
    "# step 2, loss: 9.00924015045166\n",
    "# step 3, loss: 9.145369529724121\n",
    "# step 4, loss: 8.583311080932617\n",
    "# step 5, loss: 8.283556938171387\n",
    "# step 6, loss: 8.973587989807129\n",
    "# step 7, loss: 8.775681495666504\n",
    "# step 8, loss: 8.148415565490723\n",
    "# step 9, loss: 7.83228063583374\n",
    "# step 10, loss: 8.298928260803223\n",
    "# step 11, loss: 7.384541034698486\n",
    "# step 12, loss: 7.6895294189453125\n",
    "# step 13, loss: 7.419340133666992\n",
    "# step 14, loss: 7.567596435546875\n",
    "# step 15, loss: 7.4096784591674805\n",
    "# step 16, loss: 7.3226213455200195\n",
    "# step 17, loss: 8.266316413879395\n",
    "# step 18, loss: 7.1639533042907715\n",
    "# step 19, loss: 7.797882556915283\n",
    "# step 20, loss: 7.549393177032471\n",
    "# step 21, loss: 7.749938011169434\n",
    "# step 22, loss: 6.446366786956787\n",
    "# step 23, loss: 6.835708141326904\n",
    "# step 24, loss: 6.883994102478027\n",
    "# step 25, loss: 6.704035758972168\n",
    "# step 26, loss: 6.787974834442139\n",
    "# step 27, loss: 7.700334072113037\n",
    "# step 28, loss: 7.19415807723999\n",
    "# step 29, loss: 7.0182342529296875\n",
    "# step 30, loss: 6.9842987060546875\n",
    "# step 31, loss: 7.323225498199463\n",
    "# step 32, loss: 7.172547340393066\n",
    "# step 33, loss: 7.044727325439453\n",
    "# step 34, loss: 7.917436122894287\n",
    "# step 35, loss: 7.7980523109436035\n",
    "# step 36, loss: 7.7756428718566895\n",
    "# step 37, loss: 7.733654499053955\n",
    "# step 38, loss: 8.064729690551758\n",
    "# step 39, loss: 7.523628234863281\n",
    "# step 40, loss: 7.474881172180176\n",
    "# step 41, loss: 7.044745922088623\n",
    "# step 42, loss: 7.131816387176514\n",
    "# step 43, loss: 7.149776458740234\n",
    "# step 44, loss: 7.066969394683838\n",
    "# step 45, loss: 7.148806095123291\n",
    "# step 46, loss: 6.268684387207031\n",
    "# step 47, loss: 6.4054179191589355\n",
    "# step 48, loss: 6.962234020233154\n",
    "# step 49, loss: 6.824957847595215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Fixing model Intialization\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the GPT2 and GPT3 paper are not clear about the intinilzation of the weighs. So we will look the GPT2 code.\n",
    "- Along with the intinalzation from the code. when you look at the GPT2 paper under take 2. \"we scale the weights of residual layers at initialization by a factor of 1/ sqrt(N) where N is the number of residual layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.9437)\n"
     ]
    }
   ],
   "source": [
    "# motivation on the second point on what mean in the paper. \n",
    "# you start with zero in the residual stream \\\n",
    "# if you look the code where we added the residuals \n",
    "    # def forward(self, x): of class Block\n",
    "    #     x = x + self.attn(self.ln_1(x))\n",
    "    #     x = x + self.mlp(self.ln_2(x))\n",
    "# due to this addition the varience of these activations in \n",
    "# residual stream grows \n",
    "\n",
    "x = torch.zeros(768) # this is the residual stream \n",
    "n = 100 # eg: 100 layers \n",
    "\n",
    "for i in range(n):\n",
    "    # randn: gives us the normal distribution with \n",
    "    # mean = 0 and std =1 \n",
    "    x += torch.randn(768)\n",
    "\n",
    "print(x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and due to this addtion if you all the std() at the end \n",
    "# it grows to 10.\n",
    "# and the scaling factor that they used in the paper is exactly compensates \n",
    "# for that growth. \n",
    "\n",
    "# Now, if we scale down every one of these contributions by 1/sqrt(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0184)\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(768) # this is the residual stream \n",
    "n = 100 # eg: 100 layers \n",
    "for i in range(n):\n",
    "    # randn: gives us the normal distribution with \n",
    "    # mean = 0 and std =1 \n",
    "    x += n ** -0.5 * torch.randn(768)\n",
    "print(x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, if we scale in this then you can see we got 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So this is the way to control the growth of activations inside the residual stream in the forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to initilizat the weight at the end of each block so the .c_proj(x) layer \n",
    "# so we go where have intinilized c_proj i.e \n",
    "# to do this we go to CausalSelfAttention --> __init__ \n",
    "\n",
    "        # self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "#  NANOGPT_SCALE_INIT this will act like as a flag to set the scaling in std\n",
    "            # if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "            #     std *= (2 * self.config.n_layer) ** -0.5\n",
    "# (what we discuss above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "            #     std *= (2 * self.config.n_layer) ** -0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
